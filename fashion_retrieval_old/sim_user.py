# this script is implemented to showcase simple interfaces to our RL leanring pipeline
# two functions are tested in this script:
# fc, att = captioner.compute_img_feat_batch(images)
# fc: N x 2048, att: N x 14 x 14 x2048
# and
# seq_id, sentences = captioner.gen_caption_from_feat(feat_tuple_target, feat_tuple_target)
# seq_id: N x 16, sentences: N string sentences

import torch
from captioner import captioner
import random
import numpy as np
import pickle


class SynUser:
    def __init__(self, num_train=10000):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # load trained model
        params = {'model': 'resnet101',
                  'model_root': 'captioner/neuraltalk2',
                  'att_size': 7}
        model_dir = 'caption_models'
        self.captioner_relative = captioner.Captioner(is_relative=True, model_path=model_dir, image_feat_params=params)
        self.captioner_relative.opt['use_att'] = True
        # build voc
        self.vocabSize = self.captioner_relative.get_vocab_size()

        # load pre-computed data rep.
        fc = torch.from_numpy(np.load('features/fc_feature.npz')['arr_0'])
        att = torch.from_numpy(np.load('features/att_feature.npz')['arr_0'])
        self.NUM_OBS = att.size(0)
        print('Data loading completed')
        print('fc.size', fc.size())
        print('att.size', att.size())

        # split index
        idx = torch.randperm(self.NUM_OBS)
        self.NUM_TRAIN = num_train
        self.NUM_TEST = self.NUM_OBS - self.NUM_TRAIN
        self.train_idx = idx[:self.NUM_TRAIN]
        self.test_idx = idx[self.NUM_TRAIN:]
        # train
        self.train_fc_input = fc[:self.NUM_TRAIN]
        self.train_att_input = att[:self.NUM_TRAIN]
        # test
        self.test_fc_input = fc[self.NUM_TRAIN:]
        self.test_att_input = att[self.NUM_TRAIN:]

        absolute_feature = pickle.load(open('features/256embedding.p', 'rb'))
        self.train_feature = torch.tensor(absolute_feature['train'], dtype=torch.float)
        self.test_feature = torch.tensor(absolute_feature['test'], dtype=torch.float)

        self.train_index = torch.arange(0, self.train_fc_input.size(0), dtype=torch.long)
        self.test_index = torch.arange(0, self.test_fc_input.size(0), dtype=torch.long)

        print(f'Syn User init. done!\n'
              f'# Train Images: {self.NUM_TRAIN}\n'
              f'# Test images: {self.NUM_TEST}\n'
              f'Using device: {self.device}')
        return

    def sample_idx(self, img_idx: torch.LongTensor, train_mode: bool) -> None:
        """
        change the value in "img_idx" with value from 0 ~ NUM_TRAIN-1 randomly

        :param img_idx: size(N,)
        :param train_mode:
        :return:
        """
        for i in range(img_idx.size(0)):
            img_idx[i] = random.randrange(self.NUM_TRAIN if train_mode else self.NUM_TEST)
        return

    def get_feedback(self,
                     act_idx: torch.LongTensor,
                     user_idx: torch.LongTensor,
                     train_mode: bool = True) -> torch.Tensor:
        """
        get the text feedback describing difference between proposed image and ground truth image

        text feedback is generated by "captioner_relative"

        16 is the max sentence length

        :param act_idx: size(N,) index of proposed image
        :param user_idx: size(N,) index of ground truth image
        :param train_mode:
        :return: size(N, 16) padded sequence, value is index
        """

        fc = self.train_fc_input if train_mode else self.test_fc_input
        att = self.train_att_input if train_mode else self.test_att_input

        act_fc = fc[act_idx].to(self.device)
        act_att = att[act_idx].to(self.device)
        user_fc = fc[user_idx].to(self.device)
        user_att = att[user_idx].to(self.device)

        with torch.no_grad():
            seq_label, sents_label = \
                self.captioner_relative.gen_caption_from_feat((user_fc, user_att), (act_fc, act_att))

        res = torch.zeros(seq_label.size(0), 16, dtype=torch.long)
        res[:, :seq_label.size(1)].copy_(seq_label)
        return res
